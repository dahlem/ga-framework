# Copyright (C) 2007 Dominik Dahlem <Dominik.Dahlem@cs.tcd.ie>
#  
# This file is free software; as a special exception the author gives
# unlimited permission to copy and/or distribute it, with or without 
# modifications, as long as this notice is preserved.
# 
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY, to the extent permitted by law; without even the
# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

1) General Approach
 * I used GNU autotools to maintain this project:
   autoconf version 2.61
   automake version 1.10

 * For all but the last task getopt was used to parse command-line
   parameters.
   A help message can be displayed by specifying "-?" or "-h" to the
   command-line of the executable.

 * The parallel-heat.c was implemented in a portable way. I.e., it can
   be compiled as a serial or parallel (MPI) application.

2) Configuration
 * The assignment ships with a configure script generated by
   autotools. The following options are supported:

   * --enable-debug : enables debugging information. Though, it does
     not synchronize in the case of MPI.

   * --enable-mpi : enables mpi. This requires mpi to be known to the
     system. In the HPC lab set CFLAGS=-I/usr/lib/mpich/include

3) Make
 * Installing the application was not considered. 
   * To build the project with MPI support do:

   export CFLAGS=-I/usr/lib/mpich/include
   ./configure --enable-mpi
   make

   * To build the application with debugging information do:

   ./configure --enable-debug
   make

   * Or both.

4) Assignment layout
 * The assignment contains the serial code to solve the heat equations
   in a 2D lattice (src/serial). However, it was *not* used in the
   evaluation. The implementation for the assignment is in
   src/parallel. The evaluation scripts (gnuplot), the output data
   from sample runs (heat-1/2/4/6/8.sh/pbs), and the graphs are in the
   eval folder.

   - src
      - serial
      - parallel
   - eval

 * A Doxygen configuration file is provided to generate the code
   documentation in HTML. doxygen support is integrated into the makefiles.
   Run: make doxygen-run

   - doc
      - doxygen
         - html

5) Remarks on the approach
 * The application accepts a number of command-line arguments (see
   help output) to configure the dimension of the lattice, the number
   of iterations, and the temperature sources for top, bottom, left,
   and right.

 * The lattice is partitioned and scattered to participating nodes
   without the boundary information (i.e., the heat sources). All
   boundaries are exchanged with the respective neighbouring nodes,
   excluding the top and bottom parts of the lattice. The node with
   rank 0 is responsible for collecting the partial results (gather).

 * The algorithm is generic with respect to the number of nodes
   assigned to divide the task among themselves with the condition
   that the number of rows of the lattice (without the top/bottom heat
   sources) are divisible by the number of assigned nodes. If this
   assumption is not met at runtime then the application will abort
   the execution.

 * The MPI sendrecv primitive was used to exchange the boundaries to
   the respective nodes. In order to avoid dead-locks a trick has to
   be used to first establish a communication channel between 0&1,
   2&3, etc. and then between 1&2, 3&4, etc. depending on how many
   nodes are assigned to the task.

 * Support for a single arbitrary heat-source within the bounds of the
   configured lattice. A single heat-source is identified by its
   coordinates in the lattice (i.e., one cell) via configuration
   parameters -j (x-coordinate) -k (y-coordinate).

 * The command-line arguments are parsed with getopt and verified once
   they are parsed. The verification makes sure that the dimensions
   are not negative, and that the arbitrary heat-source is within the
   configured bounds of the lattice. That means the arbitrary
   heat-source is not allowed to be within the outer boundaries. In
   case of a verification failure, the respective value is reset to
   its default value without an error message presented to the user.

6) Results
 * Setup:

   * Only one run was performed. So the data does not present any
     statistical significance. However, it does present a trend that
     allows for a consistent analysis in this case.

   * The algorithm does not check for convergence.

 * Evaluation

   * The results are in the eval folder.

   * Figure heat2D-dimIter.ps/script heat2D-dimIter.gnu: 3D plot of
     execution time (z-axis) depending on the number of iterations
     (x-axis) and the dimensions of the lattice (y-axis). For the
     purpose of the tests, only square lattices were considered. As
     expected the serial setup performs the worst over all
     configuration parameters (iteration count and dimensions)
     followed by 2, 4, 6, and 8-node setups. However, low iteration
     counts do not justify the use of MPI, because the communication
     overhead relativises the division of labour. The same is true for
     low dimensions. It can be observed that higher-order parallelism
     on a fixed problem size (e.g., 1500 iterations and 2882x2882
     lattice) yields lower gains. This is explained by the increased
     communication overhead present with increasing number of nodes.

   * Figure heat2D-TimePerDim.ps/script heat2D-ByDim.gnu: 2D plot showing
     the execution times of each setup depending on the dimensionality
     of the problem at a fixed iteration count of 1500. Same result as above.

   * Figure heat2D-TimePerIter.ps/script heat2D-ByIter.gnu: 2D plot showing
     the execution times of each setup depending on the number of iterations
     of the problem at a fixed dimension of 2882x2882. Same result as above.
